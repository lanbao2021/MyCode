{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 改进的YOLO v1\n",
    "\n",
    "backbone: ResNet_18\n",
    "\n",
    "neck: SPP\n",
    "\n",
    "head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backbone: ResNet_18\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, conv1_in, conv1_out, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(conv1_in, conv1_out, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(conv1_out)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(conv1_out, conv1_out, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(conv1_out)\n",
    "        \n",
    "        # downsample是增加x通道数用的，因为下面一层的F(x)可能会增加通道数，前面一层无法直接相加\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x # 上一层的信息\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out) # conv-1有ReLU\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out) # 注意到这里conv-2没有ReLU\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x) # 没记错的李沐课里说的是用1x1卷积来增加通道数\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out) # 注意到，联合上一层信息之后在进行ReLU\n",
    "        return out\n",
    "\n",
    "\n",
    "class Backbone(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Backbone, self).__init__()\n",
    "\n",
    "        # ResNet's Head\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # ResNet Block-1\n",
    "        self.layer1 = nn.Sequential(BasicBlock(64, 64), \n",
    "                                    BasicBlock(64, 64))\n",
    "\n",
    "        # ResNet Block-2 \n",
    "        downsample = nn.Sequential(nn.Conv2d(64, 128, kernel_size=1, stride=2, bias=False),\n",
    "                                   nn.BatchNorm2d(128))\n",
    "\n",
    "        self.layer2 = nn.Sequential(BasicBlock(64, 128, 2, downsample=downsample), \n",
    "                                    BasicBlock(128, 128))\n",
    "\n",
    "        # ResNet Block-3\n",
    "        downsample = nn.Sequential(nn.Conv2d(128, 256, kernel_size=1, stride=2, bias=False),\n",
    "                                   nn.BatchNorm2d(256))\n",
    "        self.layer3 = nn.Sequential(BasicBlock(128, 256, 2, downsample=downsample), \n",
    "                                    BasicBlock(256, 256))\n",
    "\n",
    "        # ResNet Block-4\n",
    "        downsample = nn.Sequential(nn.Conv2d(256, 512, kernel_size=1, stride=2, bias=False),\n",
    "                                   nn.BatchNorm2d(512))\n",
    "        self.layer4 = nn.Sequential(BasicBlock(256, 512, 2, downsample=downsample), \n",
    "                                    BasicBlock(512, 512))\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        C_1 = self.conv1(x)\n",
    "        C_1 = self.bn1(C_1)\n",
    "        C_1 = self.relu(C_1)\n",
    "        C_1 = self.maxpool(C_1)\n",
    "\n",
    "        C_2 = self.layer1(C_1)\n",
    "        C_3 = self.layer2(C_2)\n",
    "        C_4 = self.layer3(C_3)\n",
    "        C_5 = self.layer4(C_4)\n",
    "\n",
    "        return C_5\n",
    "\n",
    "\n",
    "\n",
    "# model = Backbone()\n",
    "# #model.load_state_dict(torch.load('resnet18.pth'), strict=False)\n",
    "# # print(model) # 打印模型的网络结构\n",
    "# pre = torch.load('resnet18.pth') # 下载地址：https://download.pytorch.org/models/resnet18-5c106cde.pth\n",
    "# pre = [k for k, v in pre.items()]\n",
    "# # print(pre)\n",
    "# model.state_dict().keys()\n",
    "# model.load_state_dict(torch.load('resnet18.pth'), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neck: SPP\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Neck(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Neck, self).__init__()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 注：max_pool并没有可以学习的参数\n",
    "        # backbone ouput size: 13*13*512\n",
    "        # output size: (13-5+2*2)/1 + 1 = 13, channel=512\n",
    "        x_1 = torch.nn.functional.max_pool2d(x, 5, stride=1, padding=2)\n",
    "        # output size: (13-9+2*4)/1 + 1 = 13, channel=512\n",
    "        x_2 = torch.nn.functional.max_pool2d(x, 9, stride=1, padding=4)\n",
    "        # output size: (13-13+2*6)/1 + 1 = 13, channel=512\n",
    "        x_3 = torch.nn.functional.max_pool2d(x, 13, stride=1, padding=6)\n",
    "        # output size: 13*13, channel=512+512+512+512=2048\n",
    "        x = torch.cat([x, x_1, x_2, x_3], dim=1) \n",
    "\n",
    "        # SPP -> Detection Head还需要用1*1卷积降维到512个Channel\n",
    "        spp_to_head = nn.Sequential(nn.Conv2d(2048, 512, 1),\n",
    "                                    nn.BatchNorm2d(512),\n",
    "                                    nn.LeakyReLU(0.1, inplace=True))\n",
    "\n",
    "        return spp_to_head(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detection Head\n",
    "class Head(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Head, self).__init__()\n",
    "        # SPP output size: 13*13*512\n",
    "        # size: 13*13, channel: 256\n",
    "        head1 = nn.Sequential(nn.Conv2d(512, 256, 1, stride=1),\n",
    "                                    nn.BatchNorm2d(256),\n",
    "                                    nn.LeakyReLU(0.1, inplace=True))\n",
    "\n",
    "        # size: (13-3+2*1)/1 + 1 = 13, channel: 512\n",
    "        head2 = nn.Sequential(nn.Conv2d(256, 512, 3, stride=1, padding=1),\n",
    "                                    nn.BatchNorm2d(512),\n",
    "                                    nn.LeakyReLU(0.1, inplace=True))\n",
    "\n",
    "        # size: 13*13, channel: 256\n",
    "        head3 = nn.Sequential(nn.Conv2d(512, 256, 1, stride=1),\n",
    "                                    nn.BatchNorm2d(256),\n",
    "                                    nn.LeakyReLU(0.1, inplace=True))\n",
    "\n",
    "\n",
    "        # size: (13-3+2*1)/1 + 1 = 13, channel: 512\n",
    "        head4 = nn.Sequential(nn.Conv2d(256, 512, 3, stride=1, padding=1),\n",
    "                                    nn.BatchNorm2d(512),\n",
    "                                    nn.LeakyReLU(0.1, inplace=True))\n",
    "\n",
    "        pred = nn.Conv2d(512, 1 + 20 + 4, 1) # output [1, 25, 13, 13]\n",
    "\n",
    "        self.head = nn.Sequential(head1, head2, head3, head4, pred)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOLOv1 Model\n",
    "class YOLOv1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(YOLOv1, self).__init__()\n",
    "\n",
    "        self.backbone = Backbone()\n",
    "        self.backbone.load_state_dict(torch.load('resnet18.pth'), strict=False)\n",
    "        self.neck = Neck()\n",
    "        self.head = Head()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        out = self.backbone(x)\n",
    "        out = self.neck(out)\n",
    "        out = self.head(out) \n",
    "        \n",
    "        return out\n",
    "\n",
    "model = YOLOv1()\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "import torch.utils.data as data\n",
    "import xml.etree.ElementTree as ET # 读取XML文件需要导入的模块（Python 3.X）\n",
    "import cv2\n",
    "import Augmentations\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "    def __init__(self):\n",
    "        self.path_train_txt = '/Users/lan/Downloads/VOCdevkit/VOC2007/ImageSets/Main/train.txt'\n",
    "        self.path_train_images = '/Users/lan/Downloads/VOCdevkit/VOC2007/JPEGImages/%s.jpg'\n",
    "        self.path_train_annotations = '/Users/lan/Downloads/VOCdevkit/VOC2007/Annotations/%s.xml'\n",
    "        self.index_images = [] # ['index_1', 'index_2', ...]\n",
    "        for line in open(self.path_train_txt):\n",
    "            self.index_images.append(line.strip())\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.index_images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\" return (image, ground_truth) \"\"\"\n",
    "\n",
    "        self.image_index = self.index_images[index]\n",
    "        self.ground_truth_xml = ET.parse(self.path_train_annotations % self.image_index).getroot()\n",
    "        self.image = cv2.imread(self.path_train_images % self.image_index)\n",
    "        height, width, channels = self.image.shape\n",
    "\n",
    "        # xml标注文件中每个Object都有其class和[xmin, ymin, xmax, ymax]数据，我们需要读取出来\n",
    "        VOC_CLASSES_NAME = ('aeroplane', 'bicycle', 'bird', 'boat',\n",
    "                            'bottle', 'bus', 'car', 'cat', 'chair',\n",
    "                            'cow', 'diningtable', 'dog', 'horse',\n",
    "                            'motorbike', 'person', 'pottedplant',\n",
    "                            'sheep', 'sofa', 'train', 'tvmonitor')\n",
    "        self.class_name_to_index = dict(zip(VOC_CLASSES_NAME, range(len(VOC_CLASSES_NAME))))\n",
    "        self.xmin, self.ymin, self.xmax, self.ymax, self.index_class_name = 0, 0, 0, 0, 0\n",
    "        self.ground_truth = []\n",
    "\n",
    "        # 从xml标注文件中读取[[xmin, ymin, xmax, ymax, index_class_name], ... ]\n",
    "        for item in self.ground_truth_xml.iter('object'):\n",
    "            \n",
    "            class_name = item.find('name').text.lower().strip()\n",
    "            self.index_class_name = self.class_name_to_index[class_name]\n",
    "\n",
    "            bounding_box = item.find('bndbox')\n",
    "            # 因为图像会进行归一化，所以矩形框也要跟着变\n",
    "            # -1是因为xml文件中的坐标是从1开始的，而图像坐标是从左上角的0开始\n",
    "            self.xmin = (int(bounding_box.find('xmin').text) - 1) / width # 要不要float我还不确定\n",
    "            self.ymin = (int(bounding_box.find('ymin').text) - 1) / height\n",
    "            self.xmax = (int(bounding_box.find('xmax').text) - 1) / width\n",
    "            self.ymax = (int(bounding_box.find('ymax').text) - 1) / height\n",
    "\n",
    "            # [[xmin, ymin, xmax, ymax, index_class_name], ... ]\n",
    "            self.ground_truth.append([self.xmin, self.ymin, self.xmax, self.ymax, self.index_class_name])\n",
    "        \n",
    "        # 对图像做数据增强\n",
    "        if len(self.ground_truth) == 0:\n",
    "            self.ground_truth = np.zeros([1, 5]) # 确定是否有目标\n",
    "        else:\n",
    "            self.ground_truth = np.array(self.ground_truth) # 有的话转成np.array数组\n",
    "        \n",
    "        transform = Augmentations.SSDAugmentation(416) # 训练数据的大小416*416，可选择输入640\n",
    "        self.image, self.bounding_box, self.index_class_name = transform(self.image, \n",
    "                                                                         self.ground_truth[:, :4],\n",
    "                                                                         self.ground_truth[:, 4])\n",
    "        \n",
    "        # cv.imread-> BGR -> RGB，与img[:,:,::-1]是等价的\n",
    "        self.image = self.image[:, :, (2, 1, 0)]                                                                            \n",
    "        \n",
    "        # bounding_box: (1,4), index_class_name:(1,) \n",
    "        # np.expand_dims(index_class_name, axis=1):(1,1)\n",
    "        # np.hstack后得到(1,5)\n",
    "        self.ground_truth = np.hstack((self.bounding_box, np.expand_dims(self.index_class_name, axis=1)))\n",
    "        \n",
    "        # permute(2, 0, 1)把通道数放到前面去(H,W,C)->(C,H,W)，相当于img = img.transpose(2, 0, 1)\n",
    "        self.image = torch.from_numpy(self.image).permute(2, 0, 1)\n",
    "\n",
    "        \n",
    "\n",
    "        return self.image, self.ground_truth, height, width \n",
    "        # 一次返回一张图片的数据和标签\n",
    "        # 注意此时self.ground_truth是一个np.array对象，标签也变成了float\n",
    "        # 举例验证的代码\n",
    "        # data = Dataset()\n",
    "        # a = data.__getitem__(1)\n",
    "        # data.ground_truth\n",
    "        # array([[ 0.43127962,  0.20664207,  0.65402844,  0.71217712, 14.        ],\n",
    "        #        [ 0.13744076,  0.26568266,  0.87914692,  1.        , 12.        ]])\n",
    "\n",
    "\n",
    "def collate_fn(data_batch): \n",
    "    # 这个data_batch是什么呢？[dataset[0],dataset[1],...,dataset[batch_size-1]]\n",
    "    # dataset[0]其实就是调用了__getitem__()方法取出一个img和一个target，组成的一个tuple\n",
    "    # sample[0]对应img，sample[1]对应target或者说label，ground truth\n",
    "    ground_truth = []\n",
    "    images = []\n",
    "    for item in data_batch:\n",
    "        images.append(item[0])\n",
    "        ground_truth.append(torch.FloatTensor(item[1]))\n",
    "\n",
    "    # 把数据\n",
    "    batch_size = len(ground_truth)\n",
    "    ground_truth = [label.tolist() for label in ground_truth]\n",
    "    \n",
    "    # 这个正样本的形状跟模型输出不一样，在loss计算时进行处理\n",
    "    ground_truth_tmp = np.zeros([batch_size, 13, 13, 1+1+4+1])\n",
    "\n",
    "    for batch_index in range(batch_size):\n",
    "        for bbox in ground_truth[batch_index]:\n",
    "            class_index = int(bbox[-1])\n",
    "            xmin, ymin, xmax, ymax = bbox[:-1]\n",
    "\n",
    "            # 计算bbox的中心点\n",
    "            c_x = (xmax + xmin) / 2 * 416\n",
    "            c_y = (ymax + ymin) / 2 * 416\n",
    "            box_w = (xmax - xmin) * 416\n",
    "            box_h = (ymax - ymin) * 416\n",
    "            \n",
    "            if box_w < 1e-4 or box_h < 1e-4:\n",
    "                break    # print('Not a valid data !!!')\n",
    "\n",
    "            # 计算中心点所在的网格坐标\n",
    "            c_x_s = c_x / 32\n",
    "            c_y_s = c_y / 32\n",
    "            grid_x = int(c_x_s)\n",
    "            grid_y = int(c_y_s)\n",
    "\n",
    "            # 计算中心点偏移量和宽高的标签\n",
    "            tx = c_x_s - grid_x\n",
    "            ty = c_y_s - grid_y\n",
    "            tw = np.log(box_w)\n",
    "            th = np.log(box_h)\n",
    "\n",
    "            # 计算边界框位置参数的损失权重\n",
    "            weight = 2.0 - (box_w / 416) * (box_h / 416)\n",
    "\n",
    "            if grid_x < 13 and grid_y < 13:\n",
    "                    ground_truth_tmp[batch_index, grid_y, grid_x, 0] = 1.0 # 有边界框的才会被置为1.0哦，后知后觉～_～\n",
    "                    ground_truth_tmp[batch_index, grid_y, grid_x, 1] = class_index\n",
    "                    ground_truth_tmp[batch_index, grid_y, grid_x, 2:6] = np.array([tx, ty, tw, th])\n",
    "                    ground_truth_tmp[batch_index, grid_y, grid_x, 6] = weight\n",
    "\n",
    "    # 注意啊，这个时候13*13的矩形就被拉成了长度为169的向量了，这是为了后面方便计算loss\n",
    "    ground_truth_tmp = ground_truth_tmp.reshape(batch_size, -1, 1+1+4+1) \n",
    "    \n",
    "    # torch.stack(images, 0)就是实现(batch_size, H, W)\n",
    "    return torch.stack(images, 0), torch.from_numpy(ground_truth_tmp).float()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 416, 416])\n",
      "torch.Size([1, 169, 7])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "[1, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lan/Documents/MyCode/Augmentations.py:241: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  mode = random.choice(self.sample_options)\n"
     ]
    }
   ],
   "source": [
    "# 测试dataloader能否正常使用\n",
    "data = Dataset()\n",
    "dataloader = torch.utils.data.DataLoader(data,\n",
    "                                         batch_size=1, \n",
    "                                         shuffle=True, \n",
    "                                         collate_fn=collate_fn,\n",
    "                                         #num_workers=2, # 该参数去掉能正常使用\n",
    "                                         pin_memory=True)\n",
    "i=0\n",
    "for iter_i, (images, ground_truth) in enumerate(dataloader):\n",
    "    print(images[0].shape)\n",
    "    print(ground_truth.shape)\n",
    "    print(ground_truth[0])\n",
    "    for label in ground_truth:\n",
    "        label.tolist()\n",
    "        print(torch.tensor([1,2]).tolist())\n",
    "\n",
    "    if i==0:\n",
    "        break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512, 13, 13])\n",
      "torch.Size([1, 512, 13, 13])\n",
      "torch.Size([1, 25, 13, 13])\n"
     ]
    }
   ],
   "source": [
    "# 测试模型的输出是否正确\n",
    "data = Dataset()\n",
    "a = data.__getitem__(1)\n",
    "data.ground_truth\n",
    "img = a[0]\n",
    "img = torch.unsqueeze(img, dim=0)\n",
    "img.shape\n",
    "model(img).shape\n",
    "img = model.backbone(img)\n",
    "print(img.shape)\n",
    "img = model.neck(img)\n",
    "print(img.shape)\n",
    "img = model.head(img)\n",
    "print(img.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss\n",
    "import torch.nn as nn\n",
    "\n",
    "def loss(model_output, ground_truth):\n",
    "    batch_size = model_output.size(0)\n",
    "\n",
    "    # [B, C, H, W] -> [B, C, H*W] -> [B, H*W, C]\n",
    "    model_output = model_output.view(batch_size, 1 + 20 + 4, -1).permute(0, 2, 1)\n",
    "\n",
    "    # 从model_output分离出objectness的置信度预测、类别class预测、bbox的txtytwth预测  \n",
    "    pred_conf = model_output[:, :, 0]\n",
    "    pred_class = model_output[:, :, 1 : (1 + 20)]\n",
    "    pred_class = pred_class.permute(0, 2, 1)\n",
    "    pred_txtytwth = model_output[:, :, (1 + 20):]\n",
    "    pred_txty = pred_txtytwth[:, :, :2]\n",
    "    pred_twth = pred_txtytwth[:, :, 2:]\n",
    "    \n",
    "\n",
    "    # 从ground_truth分离出objectness的置信度、类别class、bbox的txtytwth，以及平衡不同大小的边界框的权重weight \n",
    "    conf = ground_truth[:, :, 0]\n",
    "    class_index = ground_truth[:, :, 1].long() # .long(): convet to int\n",
    "    txty = ground_truth[:, :, 2:4]\n",
    "    twth = ground_truth[:, :, 4:6]\n",
    "    bbox_weight = ground_truth[:, :, 6]\n",
    "\n",
    "    # 计算pred_conf与conf之间的MSE_loss，因为需要设置正样本和负样本之间的权重，所以不能调包\n",
    "    # 为啥要clamp我表示不理解\n",
    "    pred_conf_clamp = torch.clamp(torch.sigmoid(pred_conf), min=1e-4, max=1.0 - 1e-4)\n",
    "    positive = (conf==1.0).float()\n",
    "    negtive = (conf==0.0).float()\n",
    "    positive_loss = positive * (pred_conf_clamp - conf) ** 2\n",
    "    negtive_loss = negtive * (pred_conf_clamp) ** 2\n",
    "    conf_loss = 5.0 * positive_loss + 1.0 * negtive_loss\n",
    "    conf_loss = torch.sum(conf_loss) / batch_size # reduction='mean'\n",
    "\n",
    "    \n",
    "    class_loss_function = nn.CrossEntropyLoss(reduction='none')\n",
    "    class_loss = class_loss_function(pred_class, class_index)\n",
    "    class_loss = torch.sum(class_loss * conf) / batch_size\n",
    "\n",
    "    txty_loss_function = nn.BCEWithLogitsLoss(reduction='none')\n",
    "    txty_loss = txty_loss_function(pred_txty, txty)\n",
    "    txty_loss = torch.sum(txty_loss, dim=-1) # tx_loss + ty_loss\n",
    "    txty_loss = torch.sum(txty_loss * conf * bbox_weight) # batch个样本的loss\n",
    "    txty_loss = txty_loss / batch_size # batch个样本的mean loss\n",
    "\n",
    "    twth_loss_function = nn.MSELoss(reduction='none')\n",
    "    twth_loss = twth_loss_function(pred_twth, twth)\n",
    "    twth_loss = torch.sum(twth_loss, dim=-1) # tw_loss + th_loss\n",
    "    twth_loss = torch.sum(twth_loss * conf * bbox_weight) # batch个样本的loss\n",
    "    twth_loss = twth_loss / batch_size # batch个样本的mean loss\n",
    "\n",
    "    bbox_loss = txty_loss + twth_loss\n",
    "\n",
    "    total_loss = conf_loss + class_loss + bbox_loss\n",
    "\n",
    "    return conf_loss, class_loss, bbox_loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lan/Documents/MyCode/Augmentations.py:241: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  mode = random.choice(self.sample_options)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conf_loss: 51.57884979248047 class_loss: 7.090444564819336 bbox_loss: 188.95045471191406 total_loss: 247.6197509765625\n",
      "conf_loss: 44.375701904296875 class_loss: 8.135169982910156 bbox_loss: 133.64157104492188 total_loss: 186.15243530273438\n",
      "conf_loss: 37.05461120605469 class_loss: 6.3460493087768555 bbox_loss: 99.28950500488281 total_loss: 142.69017028808594\n",
      "conf_loss: 31.25122833251953 class_loss: 6.0319414138793945 bbox_loss: 155.6922149658203 total_loss: 192.9753875732422\n",
      "conf_loss: 27.641010284423828 class_loss: 7.335943222045898 bbox_loss: 256.82501220703125 total_loss: 291.8019714355469\n",
      "conf_loss: 23.417400360107422 class_loss: 5.3550238609313965 bbox_loss: 186.68858337402344 total_loss: 215.4610137939453\n",
      "conf_loss: 22.41053009033203 class_loss: 6.73112678527832 bbox_loss: 146.82516479492188 total_loss: 175.96682739257812\n",
      "conf_loss: 20.7586612701416 class_loss: 7.52053165435791 bbox_loss: 198.9384765625 total_loss: 227.21766662597656\n",
      "conf_loss: 17.132646560668945 class_loss: 6.240191459655762 bbox_loss: 122.81310272216797 total_loss: 146.18594360351562\n",
      "conf_loss: 16.088346481323242 class_loss: 6.248684883117676 bbox_loss: 113.5228271484375 total_loss: 135.85986328125\n",
      "conf_loss: 14.15438175201416 class_loss: 5.846421241760254 bbox_loss: 93.25052642822266 total_loss: 113.25132751464844\n",
      "conf_loss: 16.12932777404785 class_loss: 7.752079486846924 bbox_loss: 118.50074005126953 total_loss: 142.38214111328125\n",
      "conf_loss: 15.13459587097168 class_loss: 8.207904815673828 bbox_loss: 97.85797882080078 total_loss: 121.20047760009766\n",
      "conf_loss: 14.760798454284668 class_loss: 6.899528503417969 bbox_loss: 76.17023468017578 total_loss: 97.83056640625\n",
      "conf_loss: 11.605083465576172 class_loss: 5.877256870269775 bbox_loss: 57.52201843261719 total_loss: 75.00435638427734\n",
      "conf_loss: 13.603484153747559 class_loss: 6.123902320861816 bbox_loss: 82.57415771484375 total_loss: 102.30154418945312\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/lan/Documents/MyCode/YOLOv1_improved.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lan/Documents/MyCode/YOLOv1_improved.ipynb#X11sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m         optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lan/Documents/MyCode/YOLOv1_improved.ipynb#X11sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m         optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/lan/Documents/MyCode/YOLOv1_improved.ipynb#X11sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m train()\n",
      "\u001b[1;32m/Users/lan/Documents/MyCode/YOLOv1_improved.ipynb Cell 10\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lan/Documents/MyCode/YOLOv1_improved.ipynb#X11sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m, weight_decay\u001b[39m=\u001b[39m\u001b[39m0.0001\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lan/Documents/MyCode/YOLOv1_improved.ipynb#X11sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfor\u001b[39;00m \u001b[39miter\u001b[39m, (images, ground_truth) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloader):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/lan/Documents/MyCode/YOLOv1_improved.ipynb#X11sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     model_output \u001b[39m=\u001b[39m model(images)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lan/Documents/MyCode/YOLOv1_improved.ipynb#X11sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     conf_loss, class_loss, bbox_loss, total_loss \u001b[39m=\u001b[39m loss(model_output, ground_truth)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lan/Documents/MyCode/YOLOv1_improved.ipynb#X11sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mconf_loss:\u001b[39m\u001b[39m'\u001b[39m, conf_loss\u001b[39m.\u001b[39mitem(), \u001b[39m'\u001b[39m\u001b[39mclass_loss:\u001b[39m\u001b[39m'\u001b[39m, class_loss\u001b[39m.\u001b[39mitem(), \u001b[39m'\u001b[39m\u001b[39mbbox_loss:\u001b[39m\u001b[39m'\u001b[39m, bbox_loss\u001b[39m.\u001b[39mitem(), \u001b[39m'\u001b[39m\u001b[39mtotal_loss:\u001b[39m\u001b[39m'\u001b[39m, total_loss\u001b[39m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/d2l/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/lan/Documents/MyCode/YOLOv1_improved.ipynb Cell 10\u001b[0m in \u001b[0;36mYOLOv1.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lan/Documents/MyCode/YOLOv1_improved.ipynb#X11sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbackbone(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lan/Documents/MyCode/YOLOv1_improved.ipynb#X11sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mneck(out)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/lan/Documents/MyCode/YOLOv1_improved.ipynb#X11sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhead(out) \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lan/Documents/MyCode/YOLOv1_improved.ipynb#X11sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# 将输出结果的形状进行一些调整，方便后续损失函数的计算\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lan/Documents/MyCode/YOLOv1_improved.ipynb#X11sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/d2l/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/lan/Documents/MyCode/YOLOv1_improved.ipynb Cell 10\u001b[0m in \u001b[0;36mHead.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lan/Documents/MyCode/YOLOv1_improved.ipynb#X11sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/lan/Documents/MyCode/YOLOv1_improved.ipynb#X11sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhead(x)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/d2l/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/d2l/lib/python3.9/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/d2l/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/d2l/lib/python3.9/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/d2l/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/d2l/lib/python3.9/site-packages/torch/nn/modules/conv.py:457\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 457\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/d2l/lib/python3.9/site-packages/torch/nn/modules/conv.py:453\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    450\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    451\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    452\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 453\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    454\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 模型训练框架\n",
    "import torch.optim as optim\n",
    "def train():\n",
    "    model = YOLOv1()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "    dataloader = torch.utils.data.DataLoader(Dataset(), batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "    \n",
    "    for epoch in range(1):\n",
    "        for iter, (images, ground_truth) in enumerate(dataloader):\n",
    "        \n",
    "            model_output = model(images)\n",
    "            conf_loss, class_loss, bbox_loss, total_loss = loss(model_output, ground_truth)\n",
    "            print('conf_loss:', conf_loss.item(), 'class_loss:', class_loss.item(), 'bbox_loss:', bbox_loss.item(), 'total_loss:', total_loss.item())\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['conv1.bias'], unexpected_keys=['fc.weight', 'fc.bias'])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = YOLOv1()\n",
    "model.backbone.load_state_dict(torch.load('resnet18.pth'), strict=False)\n",
    "#model.load_state_dict(torch.load('resnet18.pth'), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型预测\n",
    "def predict(image):\n",
    "    model = YOLOv1()\n",
    "    model.load_state_dict()\n",
    "    model_output = model(image)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('d2l')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "55159b417f3826aea464d5c5c2f08c53809ba760ead055a010d5c73ccf6f5953"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
